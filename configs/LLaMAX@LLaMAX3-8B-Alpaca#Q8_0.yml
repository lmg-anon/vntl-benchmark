downloader:
  repo: "mradermacher/LLaMAX3-8B-Alpaca-GGUF"
  filters: "q8_0."
  output: ".\\gguf"
  #delete_after: true

backend:
  type: "local_openai"
  chat_endpoint: False

run:
  path: ".\\koboldcpp_cu12.exe"
  arguments: "--model \".\\gguf\\LLaMAX3-8B-Alpaca.Q8_0.gguf\" --threads 5 --blasthreads 5 --contextsize 2048 --usecublas normal 0 --gpulayers 999 --highpriority --flashattention --nommap --multiuser 1"